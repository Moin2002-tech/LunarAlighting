# Doxygen Documentation - modelUtils.cpp

## Overview
Enhanced Doxygen documentation for `src/Model/modelUtils.cpp` with comprehensive mathematical representations, algorithm explanations, and practical examples.

---

## 1. `orthogonal_()` Function

### Purpose
Fills the input tensor with a (semi) orthogonal matrix using QR decomposition for optimal neural network weight initialization.

### Algorithm Overview
- **Step 1:** Generate random matrix from standard normal distribution (ùí©(0, 1))
- **Step 2:** Compute QR decomposition to extract orthogonal component
- **Step 3:** Apply phase correction based on diagonal signs
- **Step 4:** Scale result by gain parameter

### Mathematical Representation

Given tensor T ‚àà ‚Ñù^(m√ón), the initialization proceeds as:

```
1. Generate: A ‚àà ‚Ñù^(rows √ó columns) from ùí©(0, 1)

2. QR Decomposition: A = Q √ó R
   - Q ‚àà ‚Ñù^(rows √ó columns) is semi-orthogonal (Q^T Q = I)
   - R ‚àà ‚Ñù^(columns √ó columns) is upper triangular

3. Phase Correction: Q' = Q √ó diag(sign(diag(R)))
   - Ensures consistent sign patterns
   - Improves numerical stability

4. Scaling: W = gains √ó Q'
   - Final weight matrix maintains orthogonal structure
   - Property: W^T W ‚âà I (near-unitary)
```

### Key Properties
- **Orthogonality condition:** ||W^T W - I||_F ‚âà 0 (Frobenius norm)
- **Gradient flow:** Prevents vanishing/exploding gradients
- **Singular values:** Maintains œÉ‚ÇÅ ‚âà œÉ‚ÇÇ ‚âà ... ‚âà œÉ‚Çô ‚âà 1.0
- **Use cases:** Optimal for initializing recurrent and deep networks

### Parameters
- `tensor`: n-dimensional tensor (n ‚â• 2), will be reshaped to 2D if necessary
- `gains`: Multiplier scalar for weights (typically 1.0 or ‚àö2 ‚âà 1.414)

### Returns
- `torch::Tensor`: Orthogonally initialized tensor with shape preserved

---

## 2. `FlattenImpl::forward()` Function

### Purpose
Flattens a multi-dimensional tensor into a 2D tensor while preserving the batch dimension.

### Algorithm Overview
- Preserve the first dimension (batch size)
- Collapse all remaining dimensions into a single dimension
- Maintain memory layout (row-major order)

### Mathematical Representation

**Input tensor:** x ‚àà ‚Ñù^(B √ó d‚ÇÅ √ó d‚ÇÇ √ó ... √ó d‚Çô)
- B = batch size (first dimension)
- d‚ÇÅ, d‚ÇÇ, ..., d‚Çô = feature dimensions

**Output tensor:** y ‚àà ‚Ñù^(B √ó D)
- D = d‚ÇÅ √ó d‚ÇÇ √ó ... √ó d‚Çô (total flattened features)

**Transformation:** y[i, :] = vec(x[i, :, :, ..., :])
- vec() denotes vectorization operator
- Preserves element ordering (C-contiguous)

### Shape Transformation Examples
- (5, 32, 32, 3) ‚Üí (5, 3072)  [5 images, 32√ó32 RGB pixels]
- (10, 64, 64) ‚Üí (10, 4096)   [10 samples, 64√ó64 features]
- (N, 256) ‚Üí (N, 256)         [no change, already 2D]

### Key Properties
- **Bijective mapping:** No information loss
- **Element count:** Total count preserved (N_in = N_out)
- **Time complexity:** O(1) for view operation (no data copy)
- **Memory complexity:** O(B √ó D)
- **Use case:** Feature map vectorization after convolutional layers

### Parameters
- `x`: Input tensor (dimension ‚â• 1), Shape: (batch_size, dim‚ÇÅ, dim‚ÇÇ, ..., dim‚Çô)

### Returns
- `torch::Tensor`: Flattened 2D tensor, Shape: (batch_size, dim‚ÇÅ √ó dim‚ÇÇ √ó ... √ó dim‚Çô)

### Example Usage
```cpp
auto flatten = Flatten();

// Flatten CNN output
auto cnn_output = torch::rand({5, 64, 8, 8});  // batch_size=5, 64 channels, 8√ó8 spatial
auto flattened = flatten->forward(cnn_output);  // shape: (5, 4096)

// Can then feed to fully-connected layer
auto fc_input = torch::nn::Linear(4096, 128);
auto logits = fc_input(flattened);  // shape: (5, 128)
```

---

## 3. `initWeights()` Function

### Purpose
Initializes network weights and biases using orthogonal and constant initialization strategies for improved convergence and gradient flow.

### Algorithm Overview
- **Step 1:** Iterate through all network parameters
- **Step 2:** Filter non-empty parameters (size(0) ‚â† 0)
- **Step 3:** Check parameter name for type identification
- **Step 4:** Apply bias or weight initialization accordingly

### Initialization Strategies

#### For Bias Parameters
- **Condition:** Parameter name contains substring "bias"
- **Method:** Constant initialization
- **Formula:** b ‚Üê bias_gain √ó 1
- **Property:** All bias values set to single scalar
- **Typical value:** bias_gain = 0.0 (centered initialization)

#### For Weight Parameters
- **Condition:** Parameter name contains substring "weight"
- **Method:** Orthogonal initialization (via `orthogonal_()` function)
- **Formula:** W ‚Üê weight_gain √ó Q, where Q is semi-orthogonal
- **Property:** Q^T Q ‚âà I (preserves gradient magnitudes)
- **Typical value:** weight_gain = 1.0 or ‚àö2 ‚âà 1.414

### Mathematical Representation

For each parameter p ‚àà P = {W‚ÇÅ, b‚ÇÅ, W‚ÇÇ, b‚ÇÇ, ..., W‚Çô, b‚Çô}:

```
IF (name(p) contains "bias"):
  p ‚Üê bias_gain (scalar ‚àà ‚Ñù)
  
ELSE IF (name(p) contains "weight"):
  p ‚Üê weight_gain √ó Q, where Q ‚àà ‚Ñù^(m√ón) with Q^T Q = I
```

### Benefits of Orthogonal Weight Initialization
- **Gradient stability:** ||‚àáL/‚àÇp|| remains ‚âà 1.0 (prevents vanishing gradients)
- **Gradient magnitude:** ||‚àÇy/‚àÇx|| ‚âà 1.0 (prevents exploding gradients)
- **Convergence speed:** Reduces training iterations needed
- **Singular values:** Maintains œÉ‚ÇÅ ‚âà œÉ‚ÇÇ ‚âà ... ‚âà œÉ‚Çô ‚âà 1.0
- **Regularization:** Reduces internal covariate shift
- **Deep networks:** Particularly effective for RNNs and deep architectures

### Benefits of Zero Bias Initialization
- **Symmetry:** No bias toward any direction
- **Learning:** Allows network to learn bias naturally
- **Efficiency:** Reduces redundancy with weight initialization
- **Convergence:** Faster convergence in early training phase

### Network Architecture Compatibility
- **Linear/Dense layers:** ‚úì (primary use case)
- **Convolutional layers:** ‚úì (initializes conv kernels)
- **Recurrent layers:** ‚úì (recommended for stability)
- **Batch normalization:** ‚úì (biases are still useful)
- **Layer normalization:** ‚úì (typically beneficial)

### Parameters
- `parameters`: Ordered dictionary of network parameters
  - Format: {layer_name.weight, layer_name.bias, ...}
  - Type: `torch::OrderedDict<std::string, torch::Tensor>`

- `weight_gain`: Scaling factor for weight initialization
  - Range: typically [0.5, 2.0]
  - Default: 1.0 (standard orthogonal)
  - Alternative: ‚àö2 ‚âà 1.414 (ReLU networks)

- `bias_gain`: Scaling factor for bias initialization
  - Range: typically [‚àí0.1, 0.1]
  - Default: 0.0 (centered)
  - Alternative: 0.01 (small positive bias)

### Returns
- `void` (modifies parameters in-place)

### Example Usage
```cpp
// Create a simple neural network
auto model = torch::nn::Sequential(
    torch::nn::Linear(28 * 28, 128),
    torch::nn::Functional(torch::relu),
    torch::nn::Linear(128, 64),
    torch::nn::Functional(torch::relu),
    torch::nn::Linear(64, 10));

// Initialize with orthogonal weights and zero biases
initWeights(model->named_parameters(), 1.0, 0.0);

// Alternative: ReLU network with ‚àö2 gain
initWeights(model->named_parameters(), std::sqrt(2.0), 0.0);
```

---

## Documentation Standards

### Doxygen Features Used
‚úì **@brief** - Function description  
‚úì **Algorithm Overview** - Bullet points explaining steps  
‚úì **Mathematical Representation** - Formal mathematical notation  
‚úì **Key Properties** - Relevant properties and benefits  
‚úì **@param** - Parameter descriptions with types and ranges  
‚úì **@return** - Return value description  
‚úì **@note** - Important implementation notes  
‚úì **@warning** - Critical warnings  
‚úì **@example** - Code examples with usage  
‚úì **@see** - Cross-references to related functions  

### Mathematical Notation
- **Vectors/Matrices:** Bold or Greek letters (e.g., W, Q, R)
- **Set notation:** Standard mathematical set symbols (e.g., ‚àà, ‚Ñù)
- **Operations:** Matrix operations (e.g., A = Q √ó R, Q^T Q = I)
- **Dimensions:** Subscripts for tensor dimensions (e.g., d‚ÇÅ, d‚ÇÇ)
- **Properties:** Big-O notation and mathematical properties

---

## Implementation Status
‚úÖ **Complete** - All three functions fully documented with:
- Detailed algorithm explanations
- Mathematical formulas and representations
- Practical examples and use cases
- Parameter ranges and typical values
- Benefits and compatibility notes

